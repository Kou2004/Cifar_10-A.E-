{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYa_9ySshSDi"
      },
      "source": [
        "# Variational AutoEncoder\n",
        "**Description:** Convolutional Variational AutoEncoder (VAE) trained on CIFAR10 digits."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders create a non continous latent space ,which dont allow interpolations in the latent space, basically it makes the latent space sparse, but as a genertaive model we need a latent space where we need a class specific compactness ,smooth\n",
        "sampling such that we can create realistic represntation.\n",
        "Lets take an example , if we input fish and doll as input to that model ,then from their linear interpolation in the latent space (continous) then we can get mermaid as output, but if the latent space is not continous then it can be waste as loss.\n",
        "\n",
        "To remove this thing Variational Encoder comes to the picture which gives distributions as input to the model and enables soft encoding to the model ,where the autoencoder are very deterministic."
      ],
      "metadata": {
        "id": "hPspmhu_epjA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVniPZKHhSDs"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjNQ6nnPhSDu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture Behind the VAE:**\n",
        "The main difference in the architecture from the encoder that encoder outputs a mean(position of the distribution) and standard deviation(spread of the distribution) of each independent vector component. Now doing sampling from these vectors we generate the input for the decoder.\n",
        "Z= mu+sigma*epsilon\n",
        "where epsilon ~N(0,1)\n",
        "though here we are going to use log variance instead of the linear variance for the making the model numerically stable. To enable the Backpropagation ,it uses the reparameterization trick in which we dissociate the sampling layer from the model."
      ],
      "metadata": {
        "id": "GaGUSfNhkS4N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoJ-Df-_hSDx"
      },
      "source": [
        "## Create a sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2vPtK2jhSDy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4rJpcOphSDz"
      },
      "source": [
        "## Build the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrY_rWMmhSD0",
        "outputId": "17f00af6-5b76-493a-cefb-8004746998fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 32)           896       ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 64)             36928     ['conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 64)             36928     ['conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 256)                  0         ['conv2d_7[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 16)                   4112      ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " z_mean (Dense)              (None, 100)                  1700      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " z_log_var (Dense)           (None, 100)                  1700      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " sampling_2 (Sampling)       (None, 100)                  0         ['z_mean[0][0]',              \n",
            "                                                                     'z_log_var[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 100760 (393.59 KB)\n",
            "Trainable params: 100760 (393.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 100\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fEnPMehSD2"
      },
      "source": [
        "## Build the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33zb5tQdhSD3",
        "outputId": "52a92fc6-a466-4d2c-94d8-f262599c7b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               25856     \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 2, 2, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2D  (None, 4, 4, 64)          36928     \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2  (None, 8, 8, 64)          36928     \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2  (None, 16, 16, 64)        36928     \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " conv2d_transpose_12 (Conv2  (None, 32, 32, 32)        18464     \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " conv2d_transpose_13 (Conv2  (None, 32, 32, 3)         867       \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 155971 (609.26 KB)\n",
            "Trainable params: 155971 (609.26 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(2 * 2* 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((2, 2, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xcGAnC-hSD4"
      },
      "source": [
        "## Define the VAE as a `Model` with a custom `train_step`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the variational encoder , there are 2 losses associated with it : 1. KL Divergence loss( indicates the distance between the distributions of the features vectors, so it reduces the sparseness of the model) 2.Reconstruction loss( maintain the separibility between the different classes)"
      ],
      "metadata": {
        "id": "qEG4tFDpmcyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob1qrdbohSD5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2),\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu35PGOBhSD5"
      },
      "source": [
        "## Train the VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rUkqBq3hSD6",
        "outputId": "fa193593-0132-4a8c-eea4-ca82d106ebc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 9s 21ms/step - loss: 697.3987 - reconstruction_loss: 680.4186 - kl_loss: 3.7857\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 662.6628 - reconstruction_loss: 652.2198 - kl_loss: 8.3982\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 655.9579 - reconstruction_loss: 645.2083 - kl_loss: 9.9399\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 651.6111 - reconstruction_loss: 639.7203 - kl_loss: 10.4781\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 646.5018 - reconstruction_loss: 636.3240 - kl_loss: 9.6413\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 644.3215 - reconstruction_loss: 634.6954 - kl_loss: 9.3818\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 643.1227 - reconstruction_loss: 633.3215 - kl_loss: 9.2152\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 641.2776 - reconstruction_loss: 632.4590 - kl_loss: 9.0348\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 640.3860 - reconstruction_loss: 631.6464 - kl_loss: 8.9450\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 640.4448 - reconstruction_loss: 630.8789 - kl_loss: 9.0132\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 639.4995 - reconstruction_loss: 630.2952 - kl_loss: 9.0423\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 638.8606 - reconstruction_loss: 629.8096 - kl_loss: 9.0154\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 638.1142 - reconstruction_loss: 629.4507 - kl_loss: 8.9904\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 638.1560 - reconstruction_loss: 629.1851 - kl_loss: 8.9932\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 637.3108 - reconstruction_loss: 628.8682 - kl_loss: 9.0099\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 638.1200 - reconstruction_loss: 628.6838 - kl_loss: 9.0579\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 637.4690 - reconstruction_loss: 628.4196 - kl_loss: 9.0908\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 636.9624 - reconstruction_loss: 628.2863 - kl_loss: 9.1290\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 637.3164 - reconstruction_loss: 627.9440 - kl_loss: 9.1855\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 636.4030 - reconstruction_loss: 627.5736 - kl_loss: 9.2507\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 636.2692 - reconstruction_loss: 627.2202 - kl_loss: 9.2698\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 636.7518 - reconstruction_loss: 627.0536 - kl_loss: 9.2686\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 636.5764 - reconstruction_loss: 626.9666 - kl_loss: 9.2576\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 636.1175 - reconstruction_loss: 626.9428 - kl_loss: 9.2427\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.9695 - reconstruction_loss: 626.8005 - kl_loss: 9.2863\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.6090 - reconstruction_loss: 626.6815 - kl_loss: 9.2778\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 635.7496 - reconstruction_loss: 626.5956 - kl_loss: 9.3015\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 635.7676 - reconstruction_loss: 626.5664 - kl_loss: 9.3013\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 636.0959 - reconstruction_loss: 626.5422 - kl_loss: 9.3016\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 635.5487 - reconstruction_loss: 626.3557 - kl_loss: 9.3276\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.2895 - reconstruction_loss: 626.3073 - kl_loss: 9.3345\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 635.1284 - reconstruction_loss: 626.2953 - kl_loss: 9.3374\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.5517 - reconstruction_loss: 626.1595 - kl_loss: 9.3656\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.8230 - reconstruction_loss: 626.1635 - kl_loss: 9.3648\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.8461 - reconstruction_loss: 626.0873 - kl_loss: 9.3819\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.9119 - reconstruction_loss: 626.0201 - kl_loss: 9.3843\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 636.0260 - reconstruction_loss: 626.0462 - kl_loss: 9.3931\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.4462 - reconstruction_loss: 625.9731 - kl_loss: 9.4020\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.1234 - reconstruction_loss: 625.9321 - kl_loss: 9.3934\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 634.8680 - reconstruction_loss: 625.8467 - kl_loss: 9.4089\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.2616 - reconstruction_loss: 625.8417 - kl_loss: 9.4248\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 634.9846 - reconstruction_loss: 625.7307 - kl_loss: 9.4542\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.2858 - reconstruction_loss: 625.7521 - kl_loss: 9.4388\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.1502 - reconstruction_loss: 625.6505 - kl_loss: 9.4750\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.1456 - reconstruction_loss: 625.6435 - kl_loss: 9.4712\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 634.6993 - reconstruction_loss: 625.5907 - kl_loss: 9.4821\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 634.5080 - reconstruction_loss: 625.5649 - kl_loss: 9.4845\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 634.4573 - reconstruction_loss: 625.5115 - kl_loss: 9.4883\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.0777 - reconstruction_loss: 625.4576 - kl_loss: 9.4956\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 635.0048 - reconstruction_loss: 625.4658 - kl_loss: 9.5006\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x796afc665cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.cifar10.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = mnist_digits.astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=50, batch_size=256)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}